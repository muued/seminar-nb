\documentclass{article}

\usepackage{ucs} % Unicode - dependency of utf8x inputenc
\usepackage[utf8x]{inputenc} % "utf8x" uses "ucs"-package, better than "UTF8"
\usepackage[T1]{fontenc}
\usepackage{lmodern}

\usepackage{amsmath}
\usepackage[english]{babel}
\usepackage{amssymb}
\usepackage{amsxtra}
\usepackage{epsfig,psfrag}
\usepackage{listings}
\usepackage{url}

\usepackage{epstopdf}

\usepackage[table]{xcolor}
\usepackage{tikz}

\newcommand{\refchapter}[1]{Chapter~\ref{#1}}
\newcommand{\refsec}[1]{Section~\ref{#1}}
\newcommand{\refeqn}[1]{Equation~(\ref{#1})}
\newcommand{\reffig}[1]{Figure~\ref{#1}}

\title{
{\bf \scriptsize RHEINISCH-WESTF\"ALISCHE TECHNISCHE HOCHSCHULE AACHEN \\
LuFG Informatik 12 (Prof. Dr. rer. nat. Uwe Naumann)}
\vspace{.5cm} \\
\epsfig{file=figures/STCE_Logo_WWW.eps,width=.7\textwidth}
\vspace{1cm} \\
{\bf \Large Numerical Libraries} \\
{\bf \large Statistics} \\
{\large The $\chi^2$ and Kolmogorov-Smirnov tests in the NAG C Library}
}

\author{Christian Janßen (302530) \\ Fabian Ohler (280424) }
\date{July 15, 2013}

\begin{document}

\lstloadlanguages{[ISO]C++}
\lstset{basicstyle=\small, numbers=left, numberstyle=\footnotesize,
  stepnumber=1, numbersep=5pt, breaklines=true, escapeinside={/*@}{@*/}}

\begin{titlepage}
\clearpage
\maketitle
\thispagestyle{empty}
\end{titlepage}

\tableofcontents

\pagestyle{headings}
\newpage



\section{Introduction to Statistics}
\subsection{Fundamentals}
\begin{description}
	\item[sample space] collection of all possible outcomes of an experiment
	\item[P(A)] probability of the event A
	\item[random variable] function assigning real numbers to points in the sample space
	\item[probability function of a random variable X] denoted $f(x)$. Gives the probability of X assuming the value x, so $f(x) = P(X = x)$
	\item[cumulative distribution function of a random variable X] denoted $F(x)$. Gives the probability of X being less than or equal to x, so $F(x) = P(X \leq x) = \sum_{t\leq x}f(t)$
	\item[quantile $x_p$] $p$th quantile of the random variable X, if $P(X<x_p)\leq p$ and $P(X>x_p)\leq 1-p$ for $p\in[0,1]$. e.g. $x_{0.5}$ is the median.
	\item[expected value] of a real valued function $u(X)$ of a random variable X with the probability function $f(x)$, written $E[u(X)] = \sum_{x}u(x)f(x)$
	\item[mean] of a random variable X: $\mu = E(X)$
	\item[variance] of a random variable X with mean $\mu$ and the probability function $f(x)$: $\sigma^2 = E[(x-\mu)^2]$. Its positive square root is the \emph{standard deviation}.

\end{description}

\subsection{Probability Distributions}
\begin{description}
	\item[binomial distribution] $F(x) = \sum_{i\leq x} \genfrac(){0pt}{}{n}{i} p^{i}q^{n-i}$
	\item[normal distribution] $F(x) = \int_{-\infty}^{x}\frac{1}{\sqrt{2\pi\sigma}}e^{-\frac{1}{2}[(y-\mu)/\sigma]^2}\textrm{d}y $ (standard normal distribution has $\mu=0,~\sigma=1$)
	\item[uniform distribution] ...
	\item[exponential distribution] ...
	\item[$\chi^2$ distribution] with k degrees of freedom $F(x) = \left\{\begin{tabular}{ll}
		$\int_{0}^{x} \frac{y^{(k/2)-1}e^{(y/2)}}{2^{k/2}\Gamma(k/2)}$ & if $x>0$ \\
		0 & if $x\leq 0$
		\end{tabular}\right.$
		\\
		example: Let $X_1, \ldots, X_k$ be $k$ independent and identically distributed standard normal random variables. Let $Y = \sum_{i=1}^{k}X_i^2$. Then $Y$ has the $\chi^2$ distribution with $k$ degrees of freedom.
	\item[gamma distribution] ...
\end{description}

\subsection{Estimation}
\begin{description}
	\item[population] all elements under investigation
	\item[sample] collection of some elements of a population
	\item[target population] population, one wants information about
	\item[sampled population] population, one has information about
	\item[random sample] a sample from a finite population is a random sample if each of the possible samples was equally likely to be obtained
	\item[random sample of size n] is a sequence of n independent and identically distributed random variables $X_1, \ldots, X_n$
\end{description}

estimates:
quality criteria of an estimate: precision (how close to the real value) and reliability (probability for the result to be correct)
point estimates: sample mean, sample variance, ...
confidence interval: consist of \emph{interval estimate} borders and the \emph{confidence coefficient} (probability, that the interval estimator will contain the unknown population quantity)

\begin{description}
	\item[empirical distribution function] Let $X_1, \ldots, X_n$ be a random sample. The empirical distribution function $S(x)$ is a function of $x$, which equals the fraction of $X_i$s that are less than or equal to $x$ for each $x$, $-\infty<x<\infty$.
	\item[sample mean] Let $X_1, \ldots, X_n$ be a random sample. $\overline{X} = \frac{1}{n}\sum_{i=1}^{n}X_i$ is the sample mean.
	\item[sample variance] Let $X_1, \ldots, X_n$ be a random sample. $S^2 = \frac{1}{n}\sum_{i=1}^{n}(X_i-\overline{X})$ is the sample variance. The positive square root of the sample variance is the sample standard deviation $S$.
\end{description}

\subsection{Statistical Inference}

\begin{tikzpicture}[node distance = 2 cm]
	\node (t) at (2,3) {\bfseries theory};
	\node (d) at (4,1.5) {deduction};
	\node (i) at (0,1.5) {induction};
	\node (e) at (2,0) {\bfseries empiricism};
	\path
		(t) edge[bend left, ->] node[right] {} (d)
		(d) edge[bend left, ->] node[left] {} (e)
		(e) edge[bend left, ->] node[right] {} (i)
		(i) edge[bend left, ->] node[left] {} (t) ;
\end{tikzpicture}


hypothesis testing as a mean of statistical induction \\
process of inferring from a sample whether or not to accept a certain statement about the population \\
the statement itself has to be a hypothesis. \\
%
The word hypothesis may in this context be defined as
\begin{quote}
	a tentative assumption made in order to draw out and test its logical and empirical consequences \cite{merriamwebster}
\end{quote}
It can be seen as a more formal version of an \emph{educated guess}.
Using a hypothesis, one may predict e.\,g. the outcome of experiments.
Generally, a hypothesis is required to be either falsifiable or verifiable, such that it can be tested.



\subsection{statistical tests}
	null hypotheses, alternate hypotheses \\
	type I error $\alpha$, type II error $\beta$


...

\section{$\chi^2$ Tests}
\subsection{Mathematical Background}
\subsection{$\chi^2$ using the NAG C Library}
\subsection{$\chi^2$ using R}
\subsection{conclusion / comparison / ...}

\section{Kolmogorov–Smirnov Test}
In statistics the Kolmogorov-Smirnov-test is a nonparametric test which checks for equality of two probability distributions.
With this test one can check either if a given set of random samples follows a specified probability distribution or if two sets of random samples have the same probability distribution.\\
The first alternative is called ``one sample Kolmogorov-Smirnov-test'' and is a goodness of fit test. The ``two sample  Kolmogorov-Smirnov-test'' is the second alternative, but due to the limited space it will be neglected in this written elaboration.

\subsection{Mathematical Background}
In the following the proceeding of the execution of the goodness of fit test is described and will be later explained by means of an illustrative example. \\
For a random variable $X$ a set of $n$ observations ordered ascending such that $x_1 \le x_2 \le ... \le x_n$ is given. For those $n$ values the empirical distribution function $S(x)$  is determined.
The value of $S(x)$ for a given $x$ is just a simple counting of values which are less than or equal $x$ and then dividing this number by $n$. Formally written: 
$$S(x) = \frac{1}{n} \sum\limits^n_{i=1} I_{x_i\le x}~,$$
where $I_{x_i\le x}$ is the indicator function, which equals $1$ if $x_i\le x$ and equals $0$ if $x_i>x$.\\
After $S(x)$  is determined one has to formulate the null hypothesis 
$$H_0:S (x)=F_0 (x)~,$$ 
where $F_0 (x)$ is a manually chosen probability distribution.\\
The alternative hypothesis 
$$H_1:S (x)\ne F_0 (x)$$ 
means that the random variable does not follow the probability distribution $S(x)$.\\
The Kolmogorov-Smirnov test statistic is
$$D_n = \max (d_0,d_1)~,$$
where 
$$d_0=\sup_{X} |S(x)-F(x)|$$ 
and
$$d_1 = \sup_{x_i \in X} | S(x_{i-1})-F(x_i)|~,$$
with $S (x_0)$  defined as $0$. With a significance level $\alpha$, the value $D_n$ is then compared to a value $D_\alpha$.
The value $D_\alpha$ is the critical value of the Kolmogorov distribution, which will not be discussed here, can be looked up in a table if $n$ is less than or equal $35$. If $n$ is greater than $35$, then the approximate value
$$D_\alpha = \frac{ \sqrt{\ln(\frac 2 \alpha)} } {\sqrt{2n}}$$ 
is used. If $D_n>D_\alpha$ the null hypothesis $H_0$  is rejected at a significance level $\alpha$.

%\subsection{Mathematical Background}
\subsection{Example}
An illustrative example for the use of the ``one-sample-Kolmogorov-Smirnov-test'' is the following everyday situation.\\
While drinking a few wheat beer at the local pub, Paul and Franz start discussing about the height of the head of their beer.
Paul states that the head of his beer has a normal, average height, while Franz claims, the average height is normally higher.\\
Because they do not come to a conclusion, Franz takes out his measuring tape and measures the height of the heads and writes them down. Due to the obvious fact that two observations are not enough to give a statement about the average height, they order few more wheat beer, measure the height from every beer head and write it down.
After rearranging the observations into an ascending order, the following table is obtained:
\begin{table}[h]
\center
\begin{tabular}{c|c|c|c|c|c|c|c|c|c}
$x_1$	&$x_2$	&$x_3$	&$x_4$	&$x_5$	&$x_6$	&$x_7$	&$x_8$	&$x_9$	&$x_{10}$\\
\hline
1	&1,1	&1,2	&1,6	&1,7	&2,1	&2,1	&2,4	&2,4	&2,5	\\
\end{tabular}
\begin{tabular}{c|c|c|c|c|c|c|c|c|c}
$x_{11}$	&$x_{12}$	&$x_{13}$	&$x_{14}$	&$x_{15}$	&$x_{16}$	&$x_{17}$	&$x_{18}$	&$x_{19}$	&$x_{20}$\\
\hline
2,6	&2,6	&2,6	&2,7	&2,8	&3	&3,3	&3,5	&3,8	&4,2\\
\end{tabular}
\end{table}
\\
With these data available, Paul claims that the heights are normally distributed with a mathematical expectation of 3 cm and a variance of 0.7, while Franz states that it is 3.5 cm and 1, respectively.\\
The bartender, who noticed their conversation recommends them to check their statements with the ``Kolmogorov-Smirnov goodness of fit test''.
Following the suggestion they construct their hypotheses:
$$H_{Franz} : S(x) = \Phi (x|3.5,1)$$
and
$$H_{Paul} : S(x) = \Phi (x|3,0.7)~,$$
where $\Phi(x|\mu,\sigma^2)$ is the cumulative distribution function of the normal distribution and $S(x)$ is the empirical distribution function.\\
Because Paul and Franz want to proof each other wrong, their hope is, that the hypothesis of the opponent is rejected by the test.
To obtain the test statistic which is needed to potentially rejected or accept a hypotheses the values for $S(x_i)$, $\Phi (x_i|3;0,7)$ and $\Phi (x_i|3,5;1)$ are computed and written down in a table to get a clear overview.
\begin{table}[ht]
\caption{observations and their respective values}
\center
\begin{tabular}{c|c|c|c|c}
\label{tab:1}
$i$ 	& $x_i$ 	& $S(x_i)$ 	& $\Phi (x_i|3;0,7)$ 	& $\Phi (x_i|3,5;1)$ 	\\
\hline
1	&	1	&	0,05	&	0,002137	&	0,006210	\\
2	&	1,1	&	0,1	&	0,003321	&	0,008198	\\
3	&	1,2	&	0,15	&	0,005064	&	0,010724	\\
4	&	1,6	&	0,2	&	0,022750	&	0,028717	\\
5	&	1,7	&	0,25	&	0,031645	&	0,035930	\\
%6	&	2,1	&	0,35	&	0,099271	&	0,080757	\\
7	&	2,1	&	0,35	&	0,099271	&	0,080757	\\
%8	&	2,4	&	0,45	&	0,195683	&	0,135666	\\
9	&	2,4	&	0,45	&	0,195683	&	0,135666	\\
10	&	2,5	&	0,5	&	0,237525	&	0,158655	\\
11	&	2,6	&	0,65	&	0,283855	&	0,184060	\\
%12	&	2,6	&	0,65	&	0,283855	&	0,184060	\\
%13	&	2,6	&	0,65	&	0,283855	&	0,184060	\\
14	&	2,7	&	0,7	&	0,334118	&	0,211855	\\
15	&	2,8	&	0,75	&	0,387548	&	0,241964	\\
16	&	3	&	0,8	&	0,500000	&	0,308538	\\
17	&	3,3	&	0,85	&	0,665882	&	0,420740	\\
18	&	3,5	&	0,9	&	0,762475	&	0,500000	\\
19	&	3,8	&	0,95	&	0,873451	&	0,617911	\\
20	&	4,2	&	1	&	0,956762	&	0,758036	\\
\end{tabular}
\end{table}
\\
The entries $d_{\{0,1\},\{Franz,Paul\}}(x_i)$ in table ~\ref{tab:2} are short forms for $S(x_i)-\Phi (x_i|\mu ;\sigma^2)$ and $S(x_{i-1})-\Phi (x_i|\mu;\sigma^2)$ with the respective values for $\mu$ and $\sigma^2$.\\
Now that all values are wrote down, Franz and Paul only need to calculate $\max(d_0,d_1)$ for their values in order to obtain the ``Kolmogorov-Smirnov test statistic''.\\
The final values are $D_{n,Franz}=0,508036$ and $D_{n,Paul}=0,366145$, both marked on table~\ref{tab:2}.
The friends agree on a significance level $\alpha = 5\%$ and compare their values with $D_\alpha=0,294$. Because $D_{n,Franz}>D_\alpha$ and $D_{n,Paul}>D_\alpha$, both hypotheses are rejected which states that Franz and Paul were both wrong.
\begin{table}[ht]
\caption{subtracted values}
\center
\begin{tabular}{c|c|c|c|c}
\label{tab:2}
$i$ 	& $d_{0,Paul}(x_i)$ 	& $d_{1,Paul}(x_i)$ 	& $d_{0,Franz}(x_i)$ 	& $d_{1,Franz}(x_i)$ 	\\
\hline
1	&	0,047863	&	0,002137	&	0,043790	&	0,006210	\\
2	&	0,096679	&	0,046679	&	0,091802	&	0,041802	\\
..	&	...		&	...		&	...		&	...		\\
%3	&	0,144936	&	0,094936	&	0,139276	&	0,089276	\\
%4	&	0,177250	&	0,127250	&	0,171283	&	0,121283	\\
%5	&	0,218355	&	0,168355	&	0,214070	&	0,164070	\\
%6	&	0,250729	&	0,150729	&	0,269243	&	0,169243	\\
%7	&	0,250729	&	0,250729	&	0,269243	&	0,269243	\\
%8	&	0,254317	&	0,154317	&	0,314334	&	0,214334	\\
%9	&	0,254317	&	0,254317	&	0,314334	&	0,314334	\\
10	&	0,262475	&	0,212475	&	0,341345	&	0,291345	\\
11	&	0,366145	&	0,216145	&	0,465940	&	0,315940	\\
12	&\cellcolor[gray]{0.9}	0,366145	&	0,366145	&	0,465940	&	0,465940	\\
..	&	...		&	...		&	...		&	...		\\
%13	&\cellcolor[gray]{0.9}	0,366145	&	0,366145	&	0,465940	&	0,465940	\\
%14	&	0,365882	&	0,315882	&	0,488145	&	0,438145	\\
15	&	0,362452	&	0,312452	&\cellcolor[gray]{0.9}	0,508036	&	0,458036	\\
%16	&	0,300000	&	0,250000	&	0,491462	&	0,441462	\\
%17	&	0,184118	&	0,134118	&	0,429260	&	0,379260	\\
%18	&	0,137525	&	0,087525	&	0,400000	&	0,350000	\\
..	&	...		&	...		&	...		&	...		\\
19	&	0,076549	&	0,026549	&	0,332089	&	0,282089	\\
20	&	0,043238	&	0,006762	&	0,241964	&	0,191964	\\
\end{tabular}
\end{table}

This result is not too surprising, if one looks at figure~\ref{fig:1}. The blue curve describes Paul's, the orange curve Franz' cumulative distribution function. The red lines represent the value $D_n$, the maximum distance between the respective assumed function and the gray empirical distribution function.
As one can see, the empirical distribution function is far from being close to the assumed function and thus the value of $D_n$ is high.
\begin{figure}[here]
\caption{the cumulative and empirical distribution function plotted}
\center
\includegraphics[width=1.0\textwidth]{figures/diagramKSexample.png}
\label{fig:1}
\end{figure}
\subsection{Kolmogorov–Smirnov using the NAG C Library}
As seen in the example, the algorithm is quite simple but needs a few computation steps for calculating the empirical and the cumulative distribution function.
Especially for higher values of $n$ or more complex cumulative distribution functions the computational effort is too high to do it manually.\\
For this reason, there are numerical libraries that provide implementations for the ``Kolmogorov-Smirnov test''. For example the ``c'' library of the ``Numerical Algorithm Group'' - in short ``nag'' - provides reliable implementation of this goodness of fit test, although it works a little different as the procedure described above.
To use the nag c function ``nag\_1\_sample\_ks\_test'' one has to specify the following variables:
\begin{itemize}
\item $n$ -- an integer storing the number of observations
\item $x[n]$ -- an double array with observations $x_1 … x_n$
\item $dist$ -- a Nag\_Distribution specifying the theoretical null distribution
\item $par[2]$ -- an array consisting of two double entries representing parameters for the specified function e.g. $\mu$ and $\sigma^2$ for a normal cumulative distribution function.
\item $estima$ -- Nag\_ParaSupplied if $para[2]$ is given or Nag\_ParaEstimated if the parameters are to be estimated
\item $dtype$ -- specifies a Nag\_TestStatistics which will be explained later
\end{itemize}
$dtype$ defines if the calculated test statistic should either be $D_n$, $D^+_n$ or $D^-_n$.
Those statistics are used to test the null hypothesis against the following alternative hypotheses.
\begin{itemize}
\item $D_n=\max\{D^+_n,D^-_n\}$ is calculated for $H_1 $: The data does not come from the specified null distribution.\\
\item $D^+_n=\max\{S(x)-F(x),0\}$ is calculated for $H_2$: The data comes from a distribution which dominates the null distribution.\\
\item $D^-_n=\max\{F(x)-S(x),F(x_i)-S(x_{i-1})\}$ is calculated for $H_3$: The data comes from a distribution which is dominated by the null distribution.\\
\end{itemize}
A visual representation for values of $D_n^+$ and $D_n^-$ can be seen in figure~\ref{fig:2}, where the orange curve is the dominating $S(x)$ and blue is dominated by $S(x)$.

\begin{figure}[here]
\caption{Example for $H_2$ and $H_3$}
\center
\includegraphics[width=.99\textwidth]{figures/diagramKSd-d+.png}
\label{fig:2}
\end{figure}
\newpage

After specifying those parameters one can run the function and gets the following values in return.
\begin{itemize}
\item $d$ -- the calculated test statistic specified by $dtype$
\item $z$ -- the standardized  statistic $Z=D*\sqrt n$
\item $p$ -- the probability for $d$
\item $fail$ -- A NagError stating if something went wrong
\end{itemize}
$p$ describes the probability that, if the observations would actually arise from the specified null distribution, meaning that $H_0$ is correct, the value of $d$ would be this extreme. Although this value indicates that the null hypothesis may not be correct, especially for low numbers like $p<0,1$, it does not mean that if $p$ is high the null hypothesis is correct. It merely means that $p$ fails to demonstrate evidence against $H_0$.

\subsection{Kolmogorov–Smirnov using R}
\subsection{conclusion / comparison / ...}

\section{Conclusion}


\newpage
\nocite{*}
\bibliographystyle{plain}
\bibliography{report}

\end{document}

